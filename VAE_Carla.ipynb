{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c562d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clearml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m  \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mclearml\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclearml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mImage_Sampler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sampler\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clearml'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import optim\n",
    "import time\n",
    "import torch.nn.functional as  F\n",
    "\n",
    "from clearml import Dataset\n",
    "\n",
    "from Image_Sampler import Sampler\n",
    "\n",
    "\n",
    "\n",
    "from clearml import Task, Logger\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"clearML\"\n",
    "PATH = \"models/\" + MODEL_NAME\n",
    "# IMG_TRAIN = \"/disk/vanishing_data/is789/anomaly_samples/train_set/\"\n",
    "# IMG_TEST = \"/disk/vanishing_data/is789/anomaly_samples/40test/\"\n",
    "# TRAIN_ID = \"7c89dda94374478a8937be5916177f70\"\n",
    "TEST_ID = \"8ce5cdd31e8e499db2e07fc70b6136d5\"\n",
    "TRAIN_ID = \"8ce5cdd31e8e499db2e07fc70b6136d5\"\n",
    "\n",
    "\n",
    "### ClearML section\n",
    "task = Task.init(project_name=\"bogdoll/Anomaly_detection_Moritz\", task_name=\"VAE_carla\", output_uri=PATH)\n",
    "task.set_base_docker(\n",
    "            \"nvcr.io/nvidia/pytorch:21.10-py3\",\n",
    "            docker_setup_bash_script=\"apt-get update && apt-get install -y python3-opencv\",\n",
    "            docker_arguments=\"-e NVIDIA_DRIVER_CAPABILITIES=all\"  # --ipc=host\",   \n",
    "            )\n",
    "task.execute_remotely('docker', clone=False, exit_process=True) \n",
    "\n",
    "\n",
    "###lr anpassen auch die early stops\n",
    "# epoch = 2500\n",
    "# BATCH_SIZE = 32\n",
    "# imgSize=512\n",
    "# zDim=512\n",
    "# learning_rate = 1e-05 #1e-04 \n",
    "# REDUCE_THRESHOLD = [0.6,0.8]\n",
    "# layers=[32, 64, 128, 265, 512]\n",
    "\n",
    "parameters = {\n",
    "    \"epoch\" : 8000,\n",
    "    \"batch_size\" : 2,\n",
    "    \"imgSize\": 512,\n",
    "    \"zDim\": 1024,\n",
    "    \"learning_rate\" : 1e-05,\n",
    "    \"layers\" : [64, 128, 265, 512, 512, 512, 512],\n",
    "    \"reduce_threshold\" : [0.6,0.8]\n",
    "}\n",
    "\n",
    "task.connect(parameters)\n",
    "start_time = time.time()\n",
    "logger = task.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "train_data = Dataset.get(dataset_id=TRAIN_ID).get_local_copy()\n",
    "train_data = Sampler.load_Images(train_data).astype(\"float32\") / 255\n",
    "print(train_data.shape)\n",
    "\n",
    "test_data = Dataset.get(dataset_id=TEST_ID).get_local_copy()\n",
    "test_data = Sampler.load_Images(test_data).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c050c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transpose images\n",
    "\n",
    "def discardLabels(data):\n",
    "    x = 0\n",
    "    new_data = []\n",
    "    for img in data:\n",
    "#         img = img.numpy()\n",
    "        img = np.transpose(img, (2,1,0))\n",
    "        new_data.append(img)\n",
    "        x+=1\n",
    "        if x == 40:\n",
    "            return np.array(new_data)\n",
    "        \n",
    "    return np.array(new_data)\n",
    "\n",
    "train_data = torch.as_tensor(discardLabels(train_data))\n",
    "test_data = torch.as_tensor(discardLabels(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e647ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_data[14]\n",
    "img = img.numpy()\n",
    "img = np.transpose(img, (2,1,0))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51232e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply noise\n",
    "noise_set = []\n",
    "\n",
    "for img in test_data:\n",
    "    pic = img.numpy().copy()\n",
    "    pic[:,250:310,200:350] = 0.0\n",
    "    noise_set.append(pic)\n",
    "\n",
    "noise_set = np.array(noise_set)\n",
    "noise_set = torch.as_tensor(noise_set)\n",
    "noise_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## show noisy image\n",
    "\n",
    "img = noise_set[11]\n",
    "# img = img.squeeze()\n",
    "img = img.numpy()\n",
    "img = np.transpose(img, (2,1,0))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, imgChannels=3, imgSize=parameters[\"imgSize\"], zDim=parameters[\"zDim\"]):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        stride=[1,2,2,2,2,2,2]\n",
    "        out_stride=[2,2,2,2,2,2,2]\n",
    "#         in_stride=[1,2,2,2,2]\n",
    "#         out_stride=[1,2,2,2,1]\n",
    "        in_padding=[1,0,0,0,0,0,0]\n",
    "        in_trans_padding=[0,0,0,0,0,1,0]\n",
    "        out_padding=[0,0,0,0,0,1,0]\n",
    "        kernel=[3,3,3,3,3,3,3]\n",
    "#         layers=[128, 128, 128, 256, 256]\n",
    "        layers=parameters[\"layers\"]\n",
    "#         layers=[32, 64, 64, 128, 128]\n",
    "#         layers=[64, 128, 128, 128, 256]\n",
    "\n",
    "        # Initializing the 2 convolutional layers and 2 full-connected layers for the encoder\n",
    "        self.encConv1 = nn.Conv2d(in_channels=imgChannels, out_channels=layers[0], kernel_size=kernel[0], stride=stride[0], padding=in_padding[0])\n",
    "        self.encBn1 = nn.BatchNorm2d(layers[0])\n",
    "        self.encConv2 = nn.Conv2d(in_channels=layers[0], out_channels=layers[1], kernel_size=kernel[1], stride=stride[1], padding=in_padding[1])\n",
    "        self.encBn2 = nn.BatchNorm2d(layers[1])\n",
    "        self.encConv3 = nn.Conv2d(in_channels=layers[1], out_channels=layers[2], kernel_size=kernel[2], stride=stride[2], padding=in_padding[2])\n",
    "        self.encBn3 = nn.BatchNorm2d(layers[2])\n",
    "        self.encConv4 = nn.Conv2d(in_channels=layers[2], out_channels=layers[3], kernel_size=kernel[3], stride=stride[3], padding=in_padding[3])\n",
    "        self.encBn4 = nn.BatchNorm2d(layers[3])\n",
    "        self.encConv5 = nn.Conv2d(in_channels=layers[3], out_channels=layers[4], kernel_size=kernel[4], stride=stride[4], padding=in_padding[4])\n",
    "        self.encBn5 = nn.BatchNorm2d(layers[4])\n",
    "        self.encConv6 = nn.Conv2d(in_channels=layers[4], out_channels=layers[5], kernel_size=kernel[5], stride=stride[5], padding=in_padding[5])\n",
    "        self.encBn6 = nn.BatchNorm2d(layers[5])\n",
    "        self.encConv7 = nn.Conv2d(in_channels=layers[5], out_channels=layers[6], kernel_size=kernel[6], stride=stride[6], padding=in_padding[6])\n",
    "        self.encBn7 = nn.BatchNorm2d(layers[6])\n",
    "        \n",
    "        encoderDims = self.calcEncoderDims(len(layers), imgSize, kernel, in_padding, stride)\n",
    "        featureDim = layers[-1] * encoderDims[-1] * encoderDims[-1]\n",
    "        self.encFC1 = nn.Linear(featureDim, zDim)\n",
    "        self.encFC2 = nn.Linear(featureDim, zDim)\n",
    "\n",
    "        # Initializing the fully-connected layer and 2 convolutional layers for decoder\n",
    "        self.decFC1 = nn.Linear(zDim, featureDim)\n",
    "        self.decBn1 = nn.BatchNorm1d(featureDim)\n",
    "        self.decConv1 = nn.ConvTranspose2d(in_channels=layers[6], out_channels=layers[5], kernel_size=kernel[6], stride=stride[6], padding=in_trans_padding[0], output_padding=out_padding[0])\n",
    "        self.decBn2 = nn.BatchNorm2d(layers[5])\n",
    "        self.decConv2 = nn.ConvTranspose2d(in_channels=layers[5], out_channels=layers[4], kernel_size=kernel[5], stride=stride[5], padding=in_trans_padding[1], output_padding=out_padding[1])\n",
    "        self.decBn3 = nn.BatchNorm2d(layers[4])\n",
    "        self.decConv3 = nn.ConvTranspose2d(in_channels=layers[4], out_channels=layers[3], kernel_size=kernel[4], stride=stride[4], padding=in_trans_padding[2], output_padding=out_padding[2])\n",
    "        self.decBn4 = nn.BatchNorm2d(layers[3])\n",
    "        self.decConv4 = nn.ConvTranspose2d(in_channels=layers[3], out_channels=layers[2], kernel_size=kernel[3], stride=stride[3], padding=in_trans_padding[3], output_padding=out_padding[3])\n",
    "        self.decBn5 = nn.BatchNorm2d(layers[2])\n",
    "        self.decConv5 = nn.ConvTranspose2d(in_channels=layers[2], out_channels=layers[1], kernel_size=kernel[2], stride=stride[2], padding=in_trans_padding[4], output_padding=out_padding[4])\n",
    "        self.decBn6 = nn.BatchNorm2d(layers[1])\n",
    "        self.decConv6 = nn.ConvTranspose2d(in_channels=layers[1], out_channels=layers[0], kernel_size=kernel[1], stride=stride[1], padding=in_trans_padding[5], output_padding=out_padding[5])\n",
    "        self.decBn7 = nn.BatchNorm2d(layers[0])\n",
    "        self.decConv7 = nn.ConvTranspose2d(in_channels=layers[0], out_channels=imgChannels, kernel_size=kernel[0], stride=stride[0], padding=in_trans_padding[6], output_padding=out_padding[6])\n",
    "        \n",
    "        self.final_encoder_dim = None\n",
    "        \n",
    "        decoderDims = self.calcDecoderDims(len(layers), encoderDims[-1], kernel, in_padding, out_padding, stride)\n",
    "        self.printModel(layers, encoderDims, decoderDims, imgSize, imgChannels)\n",
    "\n",
    "    def calcEncoderDims(self, layer_size, imageSize, kernel, in_padding, stride):\n",
    "        newDims = [imageSize]\n",
    "        for x in range(layer_size):\n",
    "            tmpSize = int((newDims[-1]-kernel[x]+2*in_padding[x])/stride[x])+1\n",
    "            newDims.append(tmpSize)\n",
    "        newDims.pop(0)\n",
    "        return newDims\n",
    "    \n",
    "    def calcDecoderDims(self, layer_size, imageSize, kernel, in_padding, out_padding, stride, d=1):\n",
    "        newDims = [imageSize]\n",
    "        for x in range(layer_size):\n",
    "            tmpSize = (newDims[-1] - 1)*stride[x] - 2*in_padding[x] + d*(kernel[x] - 1) + out_padding[x] + 1\n",
    "            newDims.append(tmpSize)\n",
    "#         newDims.pop(0)\n",
    "        return newDims\n",
    "    \n",
    "    \n",
    "    def printModel(self, layers, encDims, decDims, imageSize, imgChannels):\n",
    "        print(\"=============\")\n",
    "        print(\"Image Flow:\")\n",
    "        print(\"Encoder:\")\n",
    "        print(f\"{imageSize}x{imageSize}x{imgChannels} (Input Image)\")\n",
    "        for x in range(len(layers)):\n",
    "            print(f\"{encDims[x]}x{encDims[x]}x{layers[x]}\")\n",
    "        \n",
    "        print(\"Decoder:\")\n",
    "        k = len(layers) - 1\n",
    "        for x in range(len(layers)):\n",
    "            print(f\"{decDims[x]}x{decDims[x]}x{layers[k]}\")\n",
    "            k = k - 1\n",
    "        print(f\"{decDims[-1]}x{decDims[-1]}x{imgChannels} (Output Image)\")\n",
    "        print(\"=============\")\n",
    "            \n",
    "        \n",
    "    def encoder(self, x):\n",
    "\n",
    "        x = F.leaky_relu(self.encConv1(x))\n",
    "        x = self.encBn1(x)\n",
    "        x = F.leaky_relu(self.encConv2(x))\n",
    "        x = self.encBn2(x)\n",
    "        x = F.leaky_relu(self.encConv3(x))\n",
    "        x = self.encBn3(x)\n",
    "        x = F.leaky_relu(self.encConv4(x))\n",
    "        x = self.encBn4(x)\n",
    "        x = F.leaky_relu(self.encConv5(x))\n",
    "        x = self.encBn5(x)\n",
    "        x = F.leaky_relu(self.encConv6(x))\n",
    "        x = self.encBn6(x)\n",
    "        x = F.leaky_relu(self.encConv7(x))\n",
    "        x = self.encBn7(x)\n",
    "        self.final_encoder_dim = np.array([x.size(1), x.size(2), x.size(3)])\n",
    "        flatten = np.prod(self.final_encoder_dim)\n",
    "\n",
    "        x = x.view(-1, flatten)\n",
    "        mu = self.encFC1(x)\n",
    "        logVar = self.encFC2(x)\n",
    "        return mu, logVar\n",
    "\n",
    "    def reparameterize(self, mu, logVar):\n",
    "\n",
    "        #Reparameterization takes in the input mu and logVar and sample the mu + std * eps\n",
    "        std = torch.exp(logVar/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps\n",
    "\n",
    "    def decoder(self, z):\n",
    "\n",
    "        x = F.leaky_relu(self.decFC1(z))\n",
    "        x = self.decBn1(x)\n",
    "        x = x.view(-1, self.final_encoder_dim[0], self.final_encoder_dim[1], self.final_encoder_dim[2])\n",
    "        x = F.leaky_relu(self.decConv1(x))\n",
    "        x = self.decBn2(x)\n",
    "        x = F.leaky_relu(self.decConv2(x))\n",
    "        x = self.decBn3(x)\n",
    "        x = F.leaky_relu(self.decConv3(x))\n",
    "        x = self.decBn4(x)\n",
    "        x = F.leaky_relu(self.decConv4(x))\n",
    "        x = self.decBn5(x)\n",
    "        x = F.leaky_relu(self.decConv5(x))\n",
    "        x = self.decBn6(x)\n",
    "        x = F.leaky_relu(self.decConv6(x))\n",
    "        x = self.decBn7(x)\n",
    "        x = torch.sigmoid(self.decConv7(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # The entire pipeline of the VAE: encoder -> reparameterization -> decoder\n",
    "        # output, mu, and logVar are returned for loss computation\n",
    "        mu, logVar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logVar)\n",
    "        out = self.decoder(z)\n",
    "        return out, mu, logVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VAE()\n",
    "# model.to(device)\n",
    "# model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891e8c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(x, recon_x, mu, log_var):\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 1024), x.view(-1, 1024), reduction = \"sum\")\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 1024), x.view(-1, 1024)) * 32 * 32\n",
    "#     Recon_loss = F.binary_cross_entropy(recon_x.view(-1, 1024), x.view(-1, 1024)) * 32 * 32 *3\n",
    "#     KLD_loss = 1 + log_var - mu.pow(2) - log_var.exp()\n",
    "#     KLD_loss = torch.sum(KLD_loss)\n",
    "#     KLD_loss *= -0.5\n",
    "#     return torch.mean(Recon_loss + KLD_loss)\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 2500), x.view(-1, 2500), reduction = \"sum\") * 32 * 32 *3\n",
    "#     Recon_loss = F.binary_cross_entropy(recon_x.view(-1, imgSize*imgSize), x.view(-1, imgSize*imgSize), reduction = \"sum\") * imgSize * imgSize *3\n",
    "    imgSize = parameters[\"imgSize\"]\n",
    "    Recon_loss = F.mse_loss(recon_x.view(-1, imgSize*imgSize), x.view(-1, imgSize*imgSize), reduction = \"sum\")\n",
    "    Recon_loss_adapted = Recon_loss * imgSize * imgSize *3\n",
    "    KLD_loss = 0.5 * torch.sum(mu.pow(2) + log_var.exp() - 1 - log_var)\n",
    "    return Recon_loss_adapted + KLD_loss, Recon_loss\n",
    "#     return Recon_loss_adapted, Recon_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753dd34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = parameters[\"learning_rate\"])\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable params: {pytorch_total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae098f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders[\"train\"] = DataLoader(dataset=train_data,\n",
    "                                          batch_size=parameters[\"batch_size\"],\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "dataloaders[\"test\"] = DataLoader(dataset=test_data,\n",
    "                                          batch_size=5,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "dataloaders[\"noise\"] = DataLoader(dataset=noise_set,\n",
    "                                          batch_size=5,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "images = next(iter(dataloaders[\"test\"]))\n",
    "plt.imshow(np.transpose(images[0], (2,1,0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3cafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalOnSet(data):\n",
    "    avg_MSE = 0\n",
    "    for img in data:\n",
    "        true_img = img.numpy()\n",
    "        img = np.array([true_img])\n",
    "        img = torch.as_tensor(img)\n",
    "        img = img.to(device)\n",
    "        out, _, _ = model(img)\n",
    "        out = out[0].detach().cpu().numpy()\n",
    "        errorMatrix = np.absolute(true_img - out)\n",
    "        errorAvg = np.sum(errorMatrix) / (errorMatrix.shape[0] * errorMatrix.shape[1] * errorMatrix.shape[2])\n",
    "        avg_MSE += errorAvg\n",
    "    return avg_MSE / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d0e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_MAE = []\n",
    "val_MAE = []\n",
    "\n",
    "reducer = []\n",
    "for cutter in parameters[\"reduce_threshold\"]:\n",
    "    reducer.append(int(parameters[\"epoch\"] * cutter))\n",
    "\n",
    "print(\"Training started!\")\n",
    "\n",
    "for e in range(1, parameters[\"epoch\"]+1):\n",
    "    for reduce in reducer:    \n",
    "        if e == reduce:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = g['lr'] / 10\n",
    "            print(\"Changed learningrate\")\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_mse = 0.0\n",
    "    for x in dataloaders[\"train\"]:\n",
    "        x = x.to(device)\n",
    "        x_recon, mu, log_var = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, mse = loss_fn(x, x_recon, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_mse += mse.item()\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_mse = 0.0\n",
    "    for x in dataloaders[\"test\"]:\n",
    "        x = x.to(device)\n",
    "        x_recon, mu, log_var = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, mse = loss_fn(x, x_recon, mu, log_var)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_mse += mse.item()\n",
    "        \n",
    "    ## logging\n",
    "    train_loss /= len(dataloaders[\"train\"].dataset)\n",
    "    val_loss /= len(dataloaders[\"test\"].dataset)\n",
    "    train_mse /= len(dataloaders[\"train\"].dataset)\n",
    "    val_mse /= len(dataloaders[\"test\"].dataset)\n",
    "#     train_losses.append(train_loss)\n",
    "#     val_losses.append(val_loss)\n",
    "    logger.report_scalar(\n",
    "        \"loss\", \"train\", iteration=e, value=train_loss)\n",
    "    logger.report_scalar(\n",
    "        \"loss\", \"validation\", iteration=e, value=val_loss)\n",
    "    logger.report_scalar(\n",
    "        \"MSE\", \"train\", iteration=e, value=train_mse)\n",
    "    logger.report_scalar(\n",
    "        \"MSE\", \"validation\", iteration=e, value=val_mse)\n",
    "\n",
    "    print(f\"Epoch {e} | Loss: {train_loss} | V_Loss: {val_loss} | MSE: {train_mse} | V_MSE: {val_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe6c14",
   "metadata": {},
   "source": [
    "## Evaluation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3976cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9514ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# end_time = time.time()\n",
    "# time_elapsed = ((end_time - start_time) / 60.0) / 60.0\n",
    "# time_elapsed = int(time_elapsed * 100000)/ 100000.0\n",
    "\n",
    "# avg_mse = evalOnSet(test_data)\n",
    "# print(avg_mse)\n",
    "\n",
    "# file = open(PATH + \"summary.txt\", \"w\")\n",
    "# file.write(\"Train loss: \" + str(train_losses[-1]) + \" Val loss: \" + str(val_losses[-1]))\n",
    "# file.write(\"\\nEpochs: \" + str(epoch))\n",
    "# file.write(\"\\nBatchSize: \" + str(BATCH_SIZE))\n",
    "# file.write(\"\\nzDim: \" + str(zDim))\n",
    "# file.write(\"\\nAvg mse on test_data: \" + str(avg_mse))\n",
    "# file.write(\"\\nTime elapsed: \" + str(time_elapsed) + \" hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce19c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,5))\n",
    "# plt.title(\"Training and Validation Loss\")\n",
    "# plt.plot(val_losses[15:],label=\"val\")\n",
    "# plt.plot(train_losses[15:],label=\"train\")\n",
    "# plt.xlabel(\"iterations\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# plt.savefig(PATH + \"loss_plot.svg\")\n",
    "# plt.savefig(PATH + \"loss_plot.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c000912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printReconError(img_in, img_out, threshold=None):\n",
    "    img_in = img_in.dot([0.07, 0.72, 0.21])\n",
    "    img_out = img_out.dot([0.07, 0.72, 0.21])\n",
    "    errorMatrix = np.absolute(img_in - img_out)\n",
    "    if not threshold == None:\n",
    "        errorMatrix[errorMatrix < threshold] = 0.0\n",
    "    errorAvg = np.sum(errorMatrix) / (errorMatrix.shape[0] * errorMatrix.shape[1])\n",
    "    print(f\"MAE: {errorAvg}\")\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,15))\n",
    "    \n",
    "    ax1.set_title(\"Original\")\n",
    "    ax1.imshow(img_in, cmap=\"gray\")\n",
    "    ax2.set_title(\"Recreation\")\n",
    "    ax2.imshow(img_out, cmap=\"gray\")\n",
    "    ax3.set_title(\"ErrorMap\")\n",
    "    ax3.imshow(errorMatrix, cmap=\"gray\")\n",
    "    fig.canvas.draw()\n",
    "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    logger.report_image(\"zAnomaly\", \"forecast\", image=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de48a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.report_text('log some text', print_console=False)\n",
    "logger.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs in random.sample(list(dataloaders[\"train\"]), 1):\n",
    "        imgs = imgs.to(device)\n",
    "#         plt.subplot(121)\n",
    "        img = imgs[0].cpu().numpy()\n",
    "        img = np.transpose(img, (2,1,0))\n",
    "#         plt.imshow(img, cmap=\"gray\")\n",
    "        \n",
    "        out, mu, logVAR = model(imgs)\n",
    "#         plt.subplot(122)\n",
    "        out = out[0].cpu().numpy()\n",
    "        out = np.transpose(out, (2,1,0))\n",
    "#         plt.imshow(out, cmap=\"gray\")\n",
    "        \n",
    "        printReconError(img, out, 0.0)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def make_prediction(dataSet, index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        imgs = torch.as_tensor(np.array([dataSet[index].numpy()]))\n",
    "        print(imgs.shape)\n",
    "        imgs = imgs.to(device)\n",
    "    #         img = np.transpose(imgs[0].cpu().numpy(), [1,2,0])\n",
    "        img = imgs[0].cpu().numpy()\n",
    "        img = np.transpose(img, (2,1,0))\n",
    "\n",
    "        out, mu, logVAR = model(imgs)\n",
    "    #         outimg = np.transpose(out[0].cpu().numpy(), [1,2,0])\n",
    "        out = out[0].cpu().numpy()\n",
    "        out = np.transpose(out, (2,1,0))\n",
    "\n",
    "        #plotting\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,15))\n",
    "        ax1.set_title(\"Original\")\n",
    "        ax1.imshow(img)\n",
    "        ax2.set_title(\"Reconstruction\")\n",
    "        ax2.imshow(out)\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b58363",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_prediction(train_data, 0)\n",
    "fig.canvas.draw()\n",
    "data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "logger.report_image(\"Prediction\", \"Train_set\", image=data)\n",
    "\n",
    "fig = make_prediction(test_data, 0)\n",
    "fig.canvas.draw()\n",
    "data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "logger.report_image(\"Prediction\", \"Test_set\", image=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VAE_env",
   "language": "python",
   "name": "vae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
