{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568e2fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clearml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m  \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mclearml\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclearml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mImage_Sampler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sampler\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clearml'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import optim\n",
    "import time\n",
    "import torch.nn.functional as  F\n",
    "\n",
    "from clearml import Dataset\n",
    "\n",
    "from Image_Sampler import Sampler\n",
    "\n",
    "\n",
    "\n",
    "from clearml import Task, Logger\n",
    "\n",
    "\n",
    "\n",
    "MODEL_NAME = \"clearML\"\n",
    "PATH = \"models/\" + MODEL_NAME\n",
    "# IMG_TRAIN = \"/disk/vanishing_data/is789/anomaly_samples/train_set/\"\n",
    "# IMG_TEST = \"/disk/vanishing_data/is789/anomaly_samples/40test/\"\n",
    "TRAIN_ID = \"8c91741059884c55b8f2dfa44a07f005\"\n",
    "# TRAIN_ID = \"834165cf80dd430b9374de0c3414c23a\" #Simon train cityscape train: cb2f41e2945247ccb906b19aab083875\n",
    "TEST_ID = \"cd75e39b0aa641fc9b7e6d6c76656627\"\n",
    "# TRAIN_ID = \"8ce5cdd31e8e499db2e07fc70b6136d5\"\n",
    "\n",
    "\n",
    "### ClearML section\n",
    "task = Task.init(project_name=\"bogdoll/Anomaly_detection_Moritz\", task_name=\"BEV_Old\", output_uri=\"https://tks-zx-01.fzi.de:8081\")\n",
    "task.set_base_docker(\n",
    "            \"nvcr.io/nvidia/pytorch:21.10-py3\",\n",
    "            docker_setup_bash_script=\"apt-get update && apt-get install -y python3-opencv\",\n",
    "            docker_arguments=\"-e NVIDIA_DRIVER_CAPABILITIES=all\"  # --ipc=host\",   \n",
    "            )\n",
    "task.execute_remotely('rtx3090', clone=False, exit_process=True) \n",
    "\n",
    "\n",
    "###lr anpassen auch die early stops\n",
    "# epoch = 2500\n",
    "# BATCH_SIZE = 32\n",
    "# imgSize=512\n",
    "# zDim=512\n",
    "# learning_rate = 1e-05 #1e-04 \n",
    "# REDUCE_THRESHOLD = [0.6,0.8]\n",
    "# layers=[32, 64, 128, 265, 512]\n",
    "\n",
    "parameters = {\n",
    "    \"epoch\" : 1000,\n",
    "    \"batch_size\" : 7,\n",
    "    \"imgSize\": 256,\n",
    "    \"zDim\": 1024,\n",
    "    \"learning_rate\" : 1e-05,\n",
    "#     \"layers\" : [64, 128, 256, 256, 512, 512, 940],\n",
    "    \"layers\" : [64, 120, 240, 480, 512],\n",
    "    \"layers_out\" : [512,256,128,64],\n",
    "    \"reduce_threshold\" : [0.6,0.8]\n",
    "}\n",
    "test_size = 20\n",
    "\n",
    "guard = int(parameters[\"epoch\"] / 10)\n",
    "if guard == 0: guard = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ca89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "train_root = Dataset.get(dataset_id=TRAIN_ID).get_local_copy()\n",
    "train_data = Sampler.load_Images(train_root).astype(\"float32\") / 255\n",
    "\n",
    "train_data, test_data = torch.utils.data.random_split(train_data, [len(train_data) - test_size, test_size])\n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "parameters[\"train_data\"] = train_data.shape\n",
    "parameters[\"test_data\"] = test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef42005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start ClearML logging\n",
    "task.connect(parameters)\n",
    "logger = task.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f619c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## transpose images\n",
    "\n",
    "def transpose_img(data):\n",
    "    x = 0\n",
    "    new_data = []\n",
    "    for img in data:\n",
    "        img = np.transpose(img, (2,1,0))\n",
    "        new_data.append(img)\n",
    "        \n",
    "    return np.array(new_data)\n",
    "\n",
    "train_data = torch.as_tensor(transpose_img(train_data))\n",
    "test_data = torch.as_tensor(transpose_img(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e80a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NUR f√ºr Simon data\n",
    "# def resize(data):\n",
    "#     resizer = transforms.Resize((256,256))\n",
    "#     tmp = []\n",
    "#     for img in data:\n",
    "#         kk = resizer(img)\n",
    "#         kk = np.array(kk)\n",
    "#         tmp.append(kk)\n",
    "        \n",
    "#     return tmp\n",
    "\n",
    "# train_data = torch.as_tensor(resize(train_data))\n",
    "# test_data = torch.as_tensor(resize(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62831e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_data[14]\n",
    "img = img.numpy()\n",
    "img = np.transpose(img, (2,1,0))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a10d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## apply noise\n",
    "# noise_set = []\n",
    "\n",
    "# for img in test_data:\n",
    "#     pic = img.numpy().copy()\n",
    "#     pic[:,250:310,200:350] = 0.0\n",
    "#     noise_set.append(pic)\n",
    "\n",
    "# noise_set = np.array(noise_set)\n",
    "# noise_set = torch.as_tensor(noise_set)\n",
    "# noise_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## show noisy image\n",
    "\n",
    "# img = noise_set[11]\n",
    "# # img = img.squeeze()\n",
    "# img = img.numpy()\n",
    "# img = np.transpose(img, (2,1,0))\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d6d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9268835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, imgChannels=3, imgSize=parameters[\"imgSize\"], zDim=parameters[\"zDim\"]):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        \n",
    "        stride=[1,2,2,2,2]\n",
    "        out_stride=[2,2,2,2,1]\n",
    "#         in_stride=[1,2,2,2,2]\n",
    "#         out_stride=[1,2,2,2,1]\n",
    "        in_padding=[3,0,0,0,0]\n",
    "        in_trans_padding=[0,0,1,0,0]\n",
    "        out_padding=[0,0,1,0,0]\n",
    "        kernel=[7,3,3,3,3]\n",
    "        kernel_out=[3,3,4,4,1]\n",
    "#         layers=[128, 128, 128, 256, 256]\n",
    "        layers=parameters[\"layers\"]\n",
    "        layers_out = parameters[\"layers_out\"]\n",
    "#         layers=[32, 64, 64, 128, 128]\n",
    "#         layers=[64, 128, 128, 128, 256]\n",
    "        \n",
    "        # Initializing the 2 convolutional layers and 2 full-connected layers for the encoder\n",
    "        self.encConv1 = nn.Conv2d(in_channels=imgChannels, out_channels=layers[0], kernel_size=kernel[0], stride=stride[0], padding=in_padding[0])\n",
    "        self.encBn1 = nn.BatchNorm2d(layers[0])\n",
    "        self.encConv2 = nn.Conv2d(in_channels=layers[0], out_channels=layers[1], kernel_size=kernel[1], stride=stride[1], padding=in_padding[1])\n",
    "        self.encBn2 = nn.BatchNorm2d(layers[1])\n",
    "        self.encConv3 = nn.Conv2d(in_channels=layers[1], out_channels=layers[2], kernel_size=kernel[2], stride=stride[2], padding=in_padding[2])\n",
    "        self.encBn3 = nn.BatchNorm2d(layers[2])\n",
    "        self.encConv4 = nn.Conv2d(in_channels=layers[2], out_channels=layers[3], kernel_size=kernel[3], stride=stride[3], padding=in_padding[3])\n",
    "        self.encBn4 = nn.BatchNorm2d(layers[3])\n",
    "        self.encConv5 = nn.Conv2d(in_channels=layers[3], out_channels=layers[4], kernel_size=kernel[4], stride=stride[4], padding=in_padding[4])\n",
    "        self.encBn5 = nn.BatchNorm2d(layers[4])\n",
    "#         self.encConv6 = nn.Conv2d(in_channels=layers[4], out_channels=layers[5], kernel_size=kernel[5], stride=stride[5], padding=in_padding[5])\n",
    "#         self.encBn6 = nn.BatchNorm2d(layers[5])\n",
    "#         self.encConv7 = nn.Conv2d(in_channels=layers[5], out_channels=layers[6], kernel_size=kernel[6], stride=stride[6], padding=in_padding[6])\n",
    "#         self.encBn7 = nn.BatchNorm2d(layers[6])\n",
    "        \n",
    "        encoderDims = self.calcEncoderDims(len(layers), imgSize, kernel, in_padding, stride)\n",
    "        featureDim = layers[-1] * encoderDims[-1] * encoderDims[-1]\n",
    "        self.encFC1 = nn.Linear(featureDim, zDim)\n",
    "\n",
    "# #         Initializing the fully-connected layer and 2 convolutional layers for decoder\n",
    "        self.decFC1 = nn.Linear(zDim, featureDim)\n",
    "        self.decBn1 = nn.BatchNorm1d(featureDim)\n",
    "        self.decConv1 = nn.ConvTranspose2d(in_channels=layers[4], out_channels=layers_out[0], kernel_size=kernel_out[0], stride=out_stride[0], padding=in_trans_padding[0], output_padding=out_padding[0])\n",
    "        self.decBn2 = nn.BatchNorm2d(layers_out[0])\n",
    "        self.decConv2 = nn.ConvTranspose2d(in_channels=layers_out[0], out_channels=layers_out[1], kernel_size=kernel_out[1], stride=out_stride[1], padding=in_trans_padding[1], output_padding=out_padding[1])\n",
    "        self.decBn3 = nn.BatchNorm2d(layers_out[1])\n",
    "        self.decConv3 = nn.ConvTranspose2d(in_channels=layers_out[1], out_channels=layers_out[2], kernel_size=kernel_out[2], stride=out_stride[2], padding=in_trans_padding[2], output_padding=out_padding[2])\n",
    "        self.decBn4 = nn.BatchNorm2d(layers_out[2])\n",
    "        self.decConv4 = nn.ConvTranspose2d(in_channels=layers_out[2], out_channels=layers_out[3], kernel_size=kernel_out[3], stride=out_stride[3], padding=in_trans_padding[3], output_padding=out_padding[3])\n",
    "        self.decBn5 = nn.BatchNorm2d(layers_out[3])\n",
    "        self.decConv5 = nn.ConvTranspose2d(in_channels=layers_out[3], out_channels=imgChannels, kernel_size=kernel_out[4], stride=out_stride[4], padding=in_trans_padding[4], output_padding=out_padding[4])\n",
    "#         self.decBn6 = nn.BatchNorm2d(layers[1])\n",
    "#         self.decConv6 = nn.ConvTranspose2d(in_channels=layers[1], out_channels=layers[0], kernel_size=kernel[1], stride=stride[1], padding=in_trans_padding[5], output_padding=out_padding[5])\n",
    "#         self.decBn7 = nn.BatchNorm2d(layers[0])\n",
    "#         self.decConv7 = nn.ConvTranspose2d(in_channels=layers[0], out_channels=imgChannels, kernel_size=kernel[0], stride=stride[0], padding=in_trans_padding[6], output_padding=out_padding[6])\n",
    "        \n",
    "        self.final_encoder_dim = None\n",
    "        \n",
    "        decoderDims = self.calcDecoderDims(len(layers), encoderDims[-1], kernel_out, in_trans_padding, out_padding, out_stride)\n",
    "        self.printModel(layers, layers_out, encoderDims, decoderDims, imgSize, imgChannels)\n",
    "\n",
    "    def calcEncoderDims(self, layer_size, imageSize, kernel, in_padding, stride):\n",
    "        newDims = [imageSize]\n",
    "        for x in range(layer_size):\n",
    "#             tmpSize = int((newDims[-1]-kernel[x]+2*in_padding[x])/stride[x])+1\n",
    "            tmpSize = int(((newDims[-1] + 2*in_padding[x]-(kernel[x]-1)-1)/stride[x])+1)\n",
    "            newDims.append(tmpSize)\n",
    "        newDims.pop(0)\n",
    "        return newDims\n",
    "    \n",
    "    def calcDecoderDims(self, layer_size, imageSize, kernel, in_trans_padding, out_padding, stride, d=1):\n",
    "        newDims = [imageSize]\n",
    "        for x in range(layer_size):            \n",
    "            tmpSize = (newDims[-1] - 1)*stride[x] - 2*in_trans_padding[x] + d*(kernel[x] - 1) + out_padding[x] + 1\n",
    "            newDims.append(tmpSize)\n",
    "#         newDims.pop(0)\n",
    "        return newDims\n",
    "    \n",
    "    \n",
    "    def printModel(self, layers, layers_out, encDims, decDims, imageSize, imgChannels):\n",
    "        print(\"=============\")\n",
    "        print(\"Image Flow:\")\n",
    "        print(\"Encoder:\")\n",
    "        print(f\"{imageSize}x{imageSize}x{imgChannels} (Input Image)\")\n",
    "        for x in range(len(layers)):\n",
    "            print(f\"{encDims[x]}x{encDims[x]}x{layers[x]}\")\n",
    "        \n",
    "        print(\"Decoder:\")\n",
    "        for x in range(len(layers_out)):\n",
    "            if x == 0:\n",
    "                print(f\"{decDims[x]}x{decDims[x]}x{layers[-1]}\")\n",
    "            print(f\"{decDims[x]}x{decDims[x]}x{layers_out[x]}\")\n",
    "        print(f\"{decDims[-1]}x{decDims[-1]}x{imgChannels} (Output Image)\")\n",
    "        print(\"=============\")\n",
    "            \n",
    "        \n",
    "    def encoder(self, x):\n",
    "#         a = \n",
    "# #         b = self.res_conv1(x)\n",
    "#         print(a.size())\n",
    "#         x = x.resize_(2,32,510,510)\n",
    "#         print(x.size())\n",
    "        x1 = F.leaky_relu(self.encConv1(x))\n",
    "        x1 = self.encBn1(x1)\n",
    "#         x = self.res_conv1(x).resize_(parameters[\"batch_size\"],512,512)\n",
    "        x2 = F.leaky_relu(self.encConv2(x1))\n",
    "        x2 = self.encBn2(x2)\n",
    "        x3 = F.leaky_relu(self.encConv3(x2))\n",
    "        x3 = self.encBn3(x3)\n",
    "        x4 = F.leaky_relu(self.encConv4(x3))\n",
    "        x4 = self.encBn4(x4)\n",
    "        x5 = F.leaky_relu(self.encConv5(x4))\n",
    "        x5 = self.encBn5(x5)\n",
    "#         x6 = F.relu(self.encConv6(x5))\n",
    "#         x6 = self.encBn6(x6)\n",
    "#         x7 = F.relu(self.encConv7(x6))\n",
    "#         x7 = self.encBn7(x7)\n",
    "        self.final_encoder_dim = np.array([x5.size(1), x5.size(2), x5.size(3)])\n",
    "        flatten = np.prod(self.final_encoder_dim)\n",
    "\n",
    "        x7 = x5.view(-1, flatten)\n",
    "        z = self.encFC1(x7)\n",
    "        \n",
    "        return z\n",
    "#         return x5\n",
    "\n",
    "\n",
    "    def decoder(self, z):\n",
    "\n",
    "        d1 = F.leaky_relu(self.decFC1(z))\n",
    "        d1 = self.decBn1(d1)\n",
    "        d1 = d1.view(-1, self.final_encoder_dim[0], self.final_encoder_dim[1], self.final_encoder_dim[2])\n",
    "        d2 = F.leaky_relu(self.decConv1(d1))\n",
    "        d2 = self.decBn2(d2)\n",
    "        d3 = F.leaky_relu(self.decConv2(d2))\n",
    "        d3 = self.decBn3(d3)\n",
    "        d4 = F.leaky_relu(self.decConv3(d3))\n",
    "        d4 = self.decBn4(d4)\n",
    "        d5 = F.leaky_relu(self.decConv4(d4))\n",
    "        d5 = self.decBn5(d5)\n",
    "#         d6 = F.relu(self.decConv5(d5))\n",
    "#         d6 = self.decBn6(d6)\n",
    "#         d7 = F.relu(self.decConv6(d6))\n",
    "#         d7 = self.decBn7(d7)\n",
    "        d8 = torch.sigmoid(self.decConv5(d5))\n",
    "        return d8\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        out = self.decoder(z)\n",
    "        return out\n",
    "    \n",
    "#     def residual(self, x, out_channels, stride=2, kernel=1, padding=1):\n",
    "#         conv = nn.Conv2d(in_channels=imgChannels, out_channels=out_channels, kernel_size=kernel, stride=stride, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e249c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = VAE()\n",
    "# model.to(device)\n",
    "# model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff67813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(x, recon_x):\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 1024), x.view(-1, 1024), reduction = \"sum\")\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 1024), x.view(-1, 1024)) * 32 * 32\n",
    "#     BC_loss = F.cross_entropy(recon_x.view(-1, 1024), x.view(-1, 1024))\n",
    "#     KLD_loss = 1 + log_var - mu.pow(2) - log_var.exp()\n",
    "#     KLD_loss = torch.sum(KLD_loss)\n",
    "#     KLD_loss *= -0.5\n",
    "#     return torch.mean(Recon_loss + KLD_loss)\n",
    "#     Recon_loss = F.mse_loss(recon_x.view(-1, 2500), x.view(-1, 2500), reduction = \"sum\") * 32 * 32 *3\n",
    "#     Recon_loss = F.binary_cross_entropy(recon_x.view(-1, imgSize*imgSize), x.view(-1, imgSize*imgSize), reduction = \"sum\") * imgSize * imgSize *3\n",
    "    imgSize = parameters[\"imgSize\"]\n",
    "    Recon_loss = F.mse_loss(recon_x.view(-1, imgSize*imgSize), x.view(-1, imgSize*imgSize), reduction = \"sum\")\n",
    "    return Recon_loss, Recon_loss\n",
    "#     return Recon_loss_adapted, Recon_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c11b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = parameters[\"learning_rate\"])\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable params: {pytorch_total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {}\n",
    "dataloaders[\"train\"] = DataLoader(dataset=train_data,\n",
    "                                          batch_size=parameters[\"batch_size\"],\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "dataloaders[\"test\"] = DataLoader(dataset=test_data,\n",
    "                                          batch_size=5,\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "# dataloaders[\"noise\"] = DataLoader(dataset=noise_set,\n",
    "#                                           batch_size=5,\n",
    "#                                           shuffle=True,\n",
    "#                                           drop_last=True)\n",
    "\n",
    "images = next(iter(dataloaders[\"test\"]))\n",
    "plt.imshow(np.transpose(images[0], (2,1,0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af434da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalOnSet(data):\n",
    "    avg_MSE = 0\n",
    "    for img in data:\n",
    "        true_img = img.numpy()\n",
    "        img = np.array([true_img])\n",
    "        img = torch.as_tensor(img)\n",
    "        img = img.to(device)\n",
    "        out, _, _ = model(img)\n",
    "        out = out[0].detach().cpu().numpy()\n",
    "        errorMatrix = np.absolute(true_img - out)\n",
    "        errorAvg = np.sum(errorMatrix) / (errorMatrix.shape[0] * errorMatrix.shape[1] * errorMatrix.shape[2])\n",
    "        avg_MSE += errorAvg\n",
    "    return avg_MSE / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b929c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def make_prediction(dataSet, index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        imgs = torch.as_tensor(np.array([dataSet[index].numpy()]))\n",
    "        imgs = imgs.to(device)\n",
    "    #         img = np.transpose(imgs[0].cpu().numpy(), [1,2,0])\n",
    "        true_img = imgs[0].cpu().numpy()\n",
    "        true_img = np.transpose(true_img, (2,1,0))\n",
    "\n",
    "        out = model(imgs)\n",
    "    #         outimg = np.transpose(out[0].cpu().numpy(), [1,2,0])\n",
    "        out = out[0].cpu().numpy()\n",
    "        out = np.transpose(out, (2,1,0))\n",
    "        errorMatrix = np.absolute(true_img - out)\n",
    "        errorAvg = np.sum(errorMatrix) / (errorMatrix.shape[0] * errorMatrix.shape[1] * errorMatrix.shape[2])\n",
    "        errorAvg = int(errorAvg * 100000)/ 100000.0\n",
    "        \n",
    "        #plotting\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,15))\n",
    "        ax1.set_title(\"Original\")\n",
    "        ax1.imshow(true_img)\n",
    "        ax2.set_title(f\"Reconstruction | MAE: {errorAvg}\")\n",
    "        ax2.imshow(out)\n",
    "        return fig\n",
    "    \n",
    "def save_prediction(dataset, index, epoch, group, title):\n",
    "    fig = make_prediction(dataset, index)\n",
    "    fig.canvas.draw()\n",
    "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    logger.report_image(group, title, iteration=epoch, image=data)\n",
    "    fig.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d63126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_snapshot(epoch=None):\n",
    "    save_prediction(train_data, 0, epoch=epoch, group=\"Snapshot_Train_set\", title=\"001\")\n",
    "    save_prediction(train_data, 1, epoch=epoch, group=\"Snapshot_Train_set\", title=\"002\")\n",
    "    save_prediction(train_data, 2, epoch=epoch, group=\"Snapshot_Train_set\", title=\"003\")\n",
    "    save_prediction(train_data, 3, epoch=epoch, group=\"Snapshot_Train_set\", title=\"004\")\n",
    "    \n",
    "    save_prediction(test_data, 0, epoch=epoch, group=\"Snapshot_Validation_set\", title=\"001\")\n",
    "    save_prediction(test_data, 1, epoch=epoch, group=\"Snapshot_Validation_set\", title=\"002\")\n",
    "    save_prediction(test_data, 2, epoch=epoch, group=\"Snapshot_Validation_set\", title=\"003\")\n",
    "    save_prediction(test_data, 3, epoch=epoch, group=\"Snapshot_Validation_set\", title=\"004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b05f3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_MAE = []\n",
    "val_MAE = []\n",
    "\n",
    "reducer = []\n",
    "for cutter in parameters[\"reduce_threshold\"]:\n",
    "    reducer.append(int(parameters[\"epoch\"] * cutter))\n",
    "\n",
    "print(\"Training started!\")\n",
    "\n",
    "for e in range(1, parameters[\"epoch\"]+1):\n",
    "    for reduce in reducer:    \n",
    "        if e == reduce:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = g['lr'] / 10\n",
    "            print(\"Changed learningrate\")\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_mse = 0.0\n",
    "    for x in dataloaders[\"train\"]:\n",
    "        x = x.to(device)\n",
    "        x_recon = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, mse = loss_fn(x, x_recon)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_mse += mse.item()\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_mse = 0.0\n",
    "    for x in dataloaders[\"test\"]:\n",
    "        x = x.to(device)\n",
    "        x_recon = model(x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, mse = loss_fn(x, x_recon)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_mse += mse.item()\n",
    "        \n",
    "    ## logging\n",
    "    train_loss /= len(dataloaders[\"train\"].dataset)\n",
    "    val_loss /= len(dataloaders[\"test\"].dataset)\n",
    "    train_mse /= len(dataloaders[\"train\"].dataset)\n",
    "    val_mse /= len(dataloaders[\"test\"].dataset)\n",
    "#     train_losses.append(train_loss)\n",
    "#     val_losses.append(val_loss)\n",
    "    logger.report_scalar(\n",
    "        \"loss\", \"train\", iteration=e, value=train_loss)\n",
    "    logger.report_scalar(\n",
    "        \"loss\", \"validation\", iteration=e, value=val_loss)\n",
    "    logger.report_scalar(\n",
    "        \"MSE\", \"train\", iteration=e, value=train_mse)\n",
    "    logger.report_scalar(\n",
    "        \"MSE\", \"validation\", iteration=e, value=val_mse)\n",
    "    if e % guard == 0 and not e == 0:\n",
    "        model.eval()\n",
    "        performance_snapshot(epoch=e)\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        model.train()\n",
    "\n",
    "    print(f\"Epoch {e} | Loss: {train_loss} | V_Loss: {val_loss} | MSE: {train_mse} | V_MSE: {val_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db42f7",
   "metadata": {},
   "source": [
    "## Evaluation Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final model\n",
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c26e1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# end_time = time.time()\n",
    "# time_elapsed = ((end_time - start_time) / 60.0) / 60.0\n",
    "# time_elapsed = int(time_elapsed * 100000)/ 100000.0\n",
    "\n",
    "# avg_mse = evalOnSet(test_data)\n",
    "# print(avg_mse)\n",
    "\n",
    "# file = open(PATH + \"summary.txt\", \"w\")\n",
    "# file.write(\"Train loss: \" + str(train_losses[-1]) + \" Val loss: \" + str(val_losses[-1]))\n",
    "# file.write(\"\\nEpochs: \" + str(epoch))\n",
    "# file.write(\"\\nBatchSize: \" + str(BATCH_SIZE))\n",
    "# file.write(\"\\nzDim: \" + str(zDim))\n",
    "# file.write(\"\\nAvg mse on test_data: \" + str(avg_mse))\n",
    "# file.write(\"\\nTime elapsed: \" + str(time_elapsed) + \" hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c49346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,5))\n",
    "# plt.title(\"Training and Validation Loss\")\n",
    "# plt.plot(val_losses[15:],label=\"val\")\n",
    "# plt.plot(train_losses[15:],label=\"train\")\n",
    "# plt.xlabel(\"iterations\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.legend()\n",
    "# plt.savefig(PATH + \"loss_plot.svg\")\n",
    "# plt.savefig(PATH + \"loss_plot.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printReconError(img_in, img_out, threshold=None):\n",
    "    img_in = img_in.dot([0.07, 0.72, 0.21])\n",
    "    img_out = img_out.dot([0.07, 0.72, 0.21])\n",
    "    errorMatrix = np.absolute(img_in - img_out)\n",
    "    if not threshold == None:\n",
    "        errorMatrix[errorMatrix < threshold] = 0.0\n",
    "    errorAvg = np.sum(errorMatrix) / (errorMatrix.shape[0] * errorMatrix.shape[1])\n",
    "    print(f\"MAE: {errorAvg}\")\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,15))\n",
    "    \n",
    "    ax1.set_title(\"Original\")\n",
    "    ax1.imshow(img_in, cmap=\"gray\")\n",
    "    ax2.set_title(\"Recreation\")\n",
    "    ax2.imshow(img_out, cmap=\"gray\")\n",
    "    ax3.set_title(\"ErrorMap\")\n",
    "    ax3.imshow(errorMatrix, cmap=\"gray\")\n",
    "    fig.canvas.draw()\n",
    "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    logger.report_image(\"zAnomaly\", \"forecast\", image=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.report_text('log some text', print_console=False)\n",
    "logger.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb36484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs in random.sample(list(dataloaders[\"train\"]), 1):\n",
    "        imgs = imgs.to(device)\n",
    "#         plt.subplot(121)\n",
    "        img = imgs[0].cpu().numpy()\n",
    "        img = np.transpose(img, (2,1,0))\n",
    "#         plt.imshow(img, cmap=\"gray\")\n",
    "        \n",
    "        out = model(imgs)\n",
    "#         plt.subplot(122)\n",
    "        out = out[0].cpu().numpy()\n",
    "        out = np.transpose(out, (2,1,0))\n",
    "#         plt.imshow(out, cmap=\"gray\")\n",
    "        \n",
    "        printReconError(img, out, 0.0)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728f9dbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mmake_prediction\u001b[49m(train_data, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw()\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mtostring_rgb(), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "save_prediction(train_data, 0, epoch=None, group=\"#Train_set\", title=\"001\")\n",
    "save_prediction(train_data, 1, epoch=None, group=\"#Train_set\", title=\"002\")\n",
    "save_prediction(train_data, 2, epoch=None, group=\"#Train_set\", title=\"003\")\n",
    "save_prediction(train_data, 3, epoch=None, group=\"#Train_set\", title=\"004\")\n",
    "\n",
    "save_prediction(test_data, 0, epoch=None, group=\"#Validation_set\", title=\"001\")\n",
    "save_prediction(test_data, 1, epoch=None, group=\"#Validation_set\", title=\"002\")\n",
    "save_prediction(test_data, 2, epoch=None, group=\"#Validation_set\", title=\"003\")\n",
    "save_prediction(test_data, 3, epoch=None, group=\"#Validation_set\", title=\"004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a57080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VAE_env",
   "language": "python",
   "name": "vae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
